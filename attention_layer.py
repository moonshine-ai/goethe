from typing import Optional
import numpy as np
import torch
from misc import identity
# TODO: reimplement these layers
from fairscale.nn.model_parallel.layers import ParallelEmbedding, RowParallelLinear, ColumnParallelLinear
# TODO: look into this class
import fairscale.nn.model_parallel.initialize as fs_init
from torch import Tensor
from args import Args
from util import apply_rotary_embedding

class AttentionLayer(torch.nn.Module):
  def __init__(self, args: Args):
    self._args = args
    self._local_heads_count = args.head_count // fs_init.get_model_parallel_world_size()
    # $W_i^Q \in \mathbb R^{d_\mathrm{model} \times d_k}$
    self._wq = ColumnParallelLinear(
      args.input_dimension,
      args.head_count * args.head_dimension,
      bias = False,
      gather_output = False,
      init_method = identity
    )

    # W_i^K \in R^{d_\mathrm{model} \times d_k}$
    self._wk = ColumnParallelLinear(
      args.input_dimension,
      args.head_count * args.head_dimension,
      bias = False,
      gather_output = False,
      init_method = identity
    )
    # W_i^V \in R^{d_\mathrm{model} \times d_k}$
    self._wv = ColumnParallelLinear(
      args.input_dimension,
      args.head_count * args.head_dimension,
      bias = False,
      gather_output = False,
      init_method = identity
    )

    # W^O \in R^{hd_v \times d_\mathrm{model}}$
    self._wo = RowParallelLinear(
      args.head_count * args.head_dimension,
      args.input_dimension,
      bias = False,
      input_is_parallel = True,
      init_method = identity
    )
    # setup caches
    self._cache_keys = torch.zeros(args.max_batch_size, args.max_sequence_length, self._local_heads_count, args.head_dimension)
    self._cache_values = torch.zeros(args.max_batch_size, args.max_sequence_length, self._local_heads_count, args.head_dimension)


    # freqs refers to the frequencies generated by $PE_{(pos, 2i)} = \sin{(\frac{pos}{10000^{2i/d_\mathrm{model}}})}
    # mask is those values that correspond to illegal connections, in order to preserve the auto-regressive property
  def forward(self, x: Tensor, start_position: int, freqs: Tensor, mask: Optional[Tensor]):
    batch_size, sequence_length = x.shape
    # get the x_q, x_k, x_v projection values from the parameter matrices
    xq, xk, xv = self._wq(x), self._wk(x), self._wv(x)
    # TODO: what do these lines do?
    # Appears to reshape the projection values into the batch size without changing the tensor size in memory
    # Why can't we store these in memory in the desired shape by default? This is done on every forward pass??
    xq, xk, xv = [ projection_value.view(batch_size, sequence_length, self.n_local_heads) for projection_value in [ xq, xk, xv] ]
    # apply rotary embedding to x_query and x_key
    # adds a positional encoding to capture relative token position in the sequence 
    xq, xk = apply_rotary_embedding(xq, xk, freqs)
    # add the new values to the cache
    self._cache_keys = self._cache_keys.to(xk)
    self._cache_values = self._cache_values.to(xv)
    self._cache_keys[:batch_size, start_position : start_position + sequence_length] = xk
    self._cache_values[:batch_size, start_position : start_position + sequence_length] = xv

    # keys and values are batch-sized entries from the start to the most recently added vectors
    keys = self._cache_keys[:batch_size, : start_position + sequence_length]
    values = self._cache_values[:batch_size, : start_position + sequence_length]

    xq = xq.transpose(1, 2)
    keys = keys.transpose(1, 2)
    values = values.transpose(1, 2)

    # scaled dot product attention
    # Attention(Q, K, V) = softmax(QK^T / sqrt{d_k})V

    scores = torch.matmul(xq, keys.transpose(2, 3)) / np.sqrt(self._args.head_dimension)
    # if we have been given a mask
    if mask is not None:
      scores += mask
    scores = F.softmax(scores.float(), dim = -1).type_as(xq)
    output = torch.matmul(scores, values)
    # TODO: what does this final transpose do?
    output = output.transpose(1, 2).contiguous().view(batch_size, sequence_length, -1)
    # MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
    # head_i = Attention(QW^Q_i, KW^K, VW^V)
    return self._wo(output)
